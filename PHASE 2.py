# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zWGOAfpvkMp-uCUdRZENEWnLWjLYCfA
"""

import pandas as pd

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.expand_frame_repr', False)

df = pd.read_csv('/content/real_2016.csv')

print(df)

df.isnull().sum()

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Assuming df is already loaded

# Remove duplicates
df.drop_duplicates(inplace=True)

# Define the feature columns (excluding the target column 'PM 2.5')
X = df[['T', 'TM', 'Tm', 'SLP', 'H', 'VV', 'V', 'VM']]

# Standardization (Z-Score Normalization)
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# Convert the standardized data back to a DataFrame
df_standardized = pd.DataFrame(X_standardized, columns=X.columns)

# Change pandas display options to show all rows
pd.set_option('display.max_rows', None)

# Print the entire standardized dataset
print("Standardized Data:\n", df_standardized)

# If you want to reset the display options after printing:
pd.reset_option('display.max_rows')

import matplotlib.pyplot as plt
import seaborn as sns

# List of numerical columns to analyze
columns = ['T', 'TM', 'Tm', 'SLP', 'H', 'VV', 'V', 'VM', 'PM 2.5']

# Set plot style
sns.set(style="whitegrid")

# Plot histograms and boxplots
for col in columns:
    plt.figure(figsize=(12, 5))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[col], kde=True, bins=30, color='skyblue')
    plt.title(f'Histogram of {col}')

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(x=df[col], color='lightgreen')
    plt.title(f'Boxplot of {col}')

    plt.tight_layout()
    plt.show()

# Scatter plots: each feature vs PM 2.5
for col in columns[:-1]:  # exclude 'PM 2.5' itself
    plt.figure(figsize=(6, 4))
    sns.scatterplot(x=df[col], y=df['PM 2.5'], color='tomato')
    plt.title(f'{col} vs PM 2.5')
    plt.xlabel(col)
    plt.ylabel('PM 2.5')
    plt.show()

# Correlation Heatmap
plt.figure(figsize=(10, 6))
corr_matrix = df[columns].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# 1. Create Humidity-to-Temperature Ratio (helps understand humidity effect on temperature)
df['H_T_ratio'] = df['H'] / (df['T'] + 1e-5)  # Avoid divide by zero by adding a small value

# 2. Create Wind Speed Impact Feature: Interaction between Visibility (VV) and Mean Wind Speed (VM)
df['VV_VM_interaction'] = df['VV'] * df['VM']

# 3. Pressure-Temperature Interaction Feature: Combining Sea-Level Pressure (SLP) and Temperature (T)
df['SLP_T_interaction'] = df['SLP'] * df['T']

# 4. Create the Temperature Difference Feature (Difference between Maximum and Minimum Temperature)
df['T_diff'] = df['T'] - df['Tm']

# 5. Create a feature for the ratio of Humidity to Wind Speed, which might highlight specific atmospheric conditions
df['H_VV_ratio'] = df['H'] / (df['VV'] + 1e-5)  # Avoid divide by zero

# 6. Ratio of Sea-Level Pressure to Visibility, which could give insight into air pressure and visibility relationships
df['SLP_VV_ratio'] = df['SLP'] / (df['VV'] + 1e-5)

# 7. Create a "Mean Wind Speed to Visibility" ratio
df['VM_VV_ratio'] = df['VM'] / (df['VV'] + 1e-5)

# To display all rows in the DataFrame
pd.set_option('display.max_rows', None)  # No limit to the number of rows displayed

# Now, print the entire DataFrame
print(df)

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Select all columns for standardization, excluding the target (e.g., PM 2.5)
# If 'PM 2.5' is your target column, you should exclude it from the scaling
features = df.drop(columns=['PM 2.5'])  # Update 'PM 2.5' with your target column name if necessary

# Fit and transform the selected features
df[features.columns] = scaler.fit_transform(features)

# To display all rows in the DataFrame
pd.set_option('display.max_rows', None)  # No limit to the number of rows displayed

# Now, print the entire DataFrame with standardized values
print(df)

from sklearn.model_selection import train_test_split

# Assume 'PM 2.5' is your target variable
X = df.drop(columns=['PM 2.5'])  # Features
y = df['PM 2.5']  # Target

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training data shape:", X_train.shape)
print("Testing data shape:", X_test.shape)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the model (Linear Regression)
model = LinearRegression()

# Train the model using the training set
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print metrics
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

import matplotlib.pyplot as plt
import seaborn as sns

# Calculate residuals
residuals = y_test - y_pred

# Plot residuals
plt.figure(figsize=(8,6))
sns.histplot(residuals, kde=True)
plt.title('Residuals Distribution')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Predicting the PM 2.5 values using the trained model
y_pred = model.predict(X_test)

# Calculate residuals
residuals = y_test - y_pred

# Plot residuals
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(0, color='r', linestyle='--')
plt.title('Residuals Plot')
plt.xlabel('Predicted PM 2.5')
plt.ylabel('Residuals')
plt.show()

# Print MAE, MSE, R2
print(f'Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred)}')
print(f'Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred)}')
print(f'R-squared (R2): {r2_score(y_test, y_pred)}')

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Predict the results
y_pred = model.predict(X_test)

# Plotting actual vs predicted values for visual comparison
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Ideal line (y=x)')
plt.title('Predicted vs Actual PM 2.5')
plt.xlabel('Actual PM 2.5')
plt.ylabel('Predicted PM 2.5')
plt.legend()
plt.show()

# Coefficients from linear regression model
coefficients = model.coef_

# Create a DataFrame for feature names and their corresponding coefficients
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': coefficients
}).sort_values(by='Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance (from Coefficients)')
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

# 1. Linear Regression Predictions
y_pred_lr = model.predict(X_test)  # Assuming 'model' is your trained Linear Regression model

# 2. Train a Random Forest Model and Make Predictions
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)  # Example random forest model
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Compare R-squared of both models
models = ['Linear Regression', 'Random Forest']
r2_scores = [r2_score(y_test, y_pred_lr), r2_score(y_test, y_pred_rf)]

# Plot the comparison
plt.figure(figsize=(8, 6))
sns.barplot(x=models, y=r2_scores)
plt.title('Model Comparison: R-squared')
plt.ylabel('R-squared')
plt.show()